{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Diversity - Goodreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('') + '/..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "from dataset_evaluation_utils import * \n",
    "from data_utils.transfer_learning_scores import *\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# import plotly.offline as py\n",
    "# pd.options.plotting.backend = \"plotly\"\n",
    "# py.init_notebook_mode() # graphs charts inline (IPython).\n",
    "\n",
    "a4_dims = (11.7, 8.27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_reclists_to_frame(filename:str, n_holdouts:int, rec_list_as_rows:bool=False):\n",
    "    '''\n",
    "        Reads all recommended list outputed by the model at the different buckets and holdouts\n",
    "        to a pd.DataFrame with bucket_idx, holdout_idx,\treclist_idx\tas the DataFrame's index\n",
    "        (does not keep the score for each item in the recommended list)\n",
    "\n",
    "        filename: expects 'path/to/file/<file_name>'\n",
    "                    string is completed with '_b<i>_h<j>.joblib' in this function\n",
    "                    full string rule : 'path/to/file/<file_name>_b<i>_h<j>.joblib'\n",
    "\n",
    "                  the file shape is assumed to be \n",
    "                    [ \n",
    "                        [ ['B001PZ06PS' '0.5036084630168064']\n",
    "                          ['B001386NZE' '0.504954758011992']\n",
    "                          ['B00PJU8VFM' '0.5300628103145297'] ... ],\n",
    "                        [ ... ], \n",
    "                      ... \n",
    "                    ]\n",
    "\n",
    "                    \n",
    "        if rec_list_as_rows: returns a pd.Dataframe like so\n",
    "                                            0\t          1\t          2          # rank of item in the Top N Rec List\n",
    "        bucket_idx\tbucket_idx\treclist_idx\t\t\t\n",
    "        b0\t        h0\t        0.0\t        B00PBW27VW  B00RDEZFN8\tB009G3S0F4 # 1st list of rec items of '<file_name>_b0_h0.joblib'\n",
    "                                1.0\t        B00PBW27VW\tB00RDEZFN8\tB009G3S0F4\n",
    "                    h1\t        0.0\t        B00SYTTBMC\tB00KJJS6HQ\tB001L0TTS2\n",
    "                                1.0\t        B00SYTTBMC\tB00KJJS6HQ\tB001L0TTS2\n",
    "                                2.0\t        B00SYTTBMC\tB00KJJS6HQ\tB001L0TTS2\n",
    "                                3.0\t        B00SYTTBMC\tB00KJJS6HQ\t<item_id>\n",
    "        else: returns\n",
    "                                              item_id\n",
    "        bucket_idx\tholdout_idx\treclist_idx\t\n",
    "        b0\t        h0\t        0\t            B001PZ06PS\n",
    "                                0\t            B001386NZE\n",
    "                                0\t            B00PJU8VFM\n",
    "                                0\t            B00PBW27VW\n",
    "                                0\t            B00RDEZFN8\n",
    "        \n",
    "    \n",
    "    '''\n",
    "        \n",
    "    fileshape = (0,20,2)\n",
    "    n = fileshape[1]\n",
    "    all_rec_lists = np.empty(fileshape, dtype=object)\n",
    "    # rec_lists_b0_h0 = joblib.load(filename+'_b0_h0.joblib')\n",
    "    # n = len(rec_lists_b0_h0[0]) # len(all_rec_lists.T[0].T[0]) would also work\n",
    "    # rs = np.empty((0, rec_lists_b0_h0.shape[1], rec_lists_b0_h0.shape[2]))\n",
    "\n",
    "    df_bucket_idxs = []\n",
    "    df_holdout_idxs = []\n",
    "    df_reclist_idxs = []\n",
    "\n",
    "\n",
    "    if rec_list_as_rows:\n",
    "\n",
    "      for bucket_i in range(n_holdouts):\n",
    "        print(bucket_i)\n",
    "        for holdout_j in range(n_holdouts):\n",
    "\n",
    "          reclists_bi_hj = np.array(joblib.load(filename+'_b'+str(bucket_i)+'_h'+str(holdout_j)+'.joblib'),  dtype=object)\n",
    "          \n",
    "          # with different len arrays, np array becomes 1d, raises an error when merging later on\n",
    "          # when 1d, it means there are empty rec lists\n",
    "          # solution, reshape those so the array has shape (20, 2)\n",
    "          if reclists_bi_hj.ndim == 1:\n",
    "            reclists_bi_hj = np.array([np.empty((20,2), dtype=object) if l==[] else l for l in reclists_bi_hj])\n",
    "\n",
    "          n_reclists = len(reclists_bi_hj)\n",
    "\n",
    "          df_bucket_idxs = np.concatenate([df_bucket_idxs, ['b'+str(bucket_i)]*n_reclists*n])\n",
    "          df_holdout_idxs = np.concatenate([df_holdout_idxs, ['h'+str(holdout_j)]*n_reclists*n])\n",
    "          df_reclist_idxs = np.concatenate([df_reclist_idxs, np.array(range(n_reclists), dtype=str)])\n",
    "\n",
    "          all_rec_lists = np.vstack([all_rec_lists, reclists_bi_hj])\n",
    "          # print(all_rec_lists)\n",
    "\n",
    "      df_idxs = np.array([df_bucket_idxs, df_holdout_idxs, df_reclist_idxs])\n",
    "      df = pd.DataFrame(all_rec_lists.T[0].T, index=list(df_idxs), columns=list(range(n)))\n",
    "      df.index.set_names(['bucket_idx', 'holdout_idx', 'reclist_idx'], inplace=True)\n",
    "    \n",
    "      print('how many rec lists are empty: (empty: True)')\n",
    "      print(df.T.isna().value_counts()/df.shape[0])\n",
    "\n",
    "    else:\n",
    "\n",
    "      for bucket_i in range(n_holdouts):\n",
    "          print(bucket_i)\n",
    "          for holdout_j in range(n_holdouts):\n",
    "            # print(bucket_i, holdout_j)\n",
    "\n",
    "            reclists_bi_hj = np.array(joblib.load(filename+'_b'+str(bucket_i)+'_h'+str(holdout_j)+'.joblib'),  dtype=object)\n",
    "            \n",
    "            # with different len arrays, np array becomes 1d, raises an error when merging later on\n",
    "            # when 1d, it means there are empty rec lists\n",
    "            # solution, reshape those so the array has shape (20, 2)\n",
    "            if reclists_bi_hj.ndim == 1:\n",
    "              reclists_bi_hj = np.array([np.empty((20,2), dtype=object) if l==[] else l for l in reclists_bi_hj])\n",
    "\n",
    "            # print(reclists_bi_hj.shape)\n",
    "            n_reclists = len(reclists_bi_hj)\n",
    "\n",
    "            df_bucket_idxs = np.concatenate([df_bucket_idxs, ['b'+str(bucket_i)]*n_reclists*n])\n",
    "            df_holdout_idxs = np.concatenate([df_holdout_idxs, ['h'+str(holdout_j)]*n_reclists*n])\n",
    "            # df_reclist_idxs = np.concatenate([df_reclist_idxs, np.array(range(n_reclists), dtype=str)])\n",
    "            df_reclist_idxs = np.concatenate([df_reclist_idxs, np.array(np.repeat(range(n_reclists), n), dtype=str)])\n",
    "\n",
    "            all_rec_lists = np.vstack([all_rec_lists, reclists_bi_hj])\n",
    "            # print(all_rec_lists)\n",
    "\n",
    "\n",
    "      df_idxs = np.array([df_bucket_idxs, df_holdout_idxs, df_reclist_idxs])\n",
    "      shape = all_rec_lists.T[0].T.shape\n",
    "      df = pd.DataFrame(all_rec_lists.T[0].T.reshape(shape[0]*shape[1]), index=list(df_idxs), columns=['item_id'])\n",
    "      df.index.set_names(['bucket_idx', 'holdout_idx', 'reclist_idx'], inplace=True)\n",
    "\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_profile_div_frame(user_div_col:str, \n",
    "                               user_profile_size_col:str, \n",
    "                               model_data:ImplicitData, \n",
    "                               bucket_idx:str, \n",
    "                               meta_df:pd.DataFrame):\n",
    "    # user_div_col = 'user_'+div_col\n",
    "    # user_profile_size_col = 'n_seen_items'\n",
    "\n",
    "    user_profile_cols = ['bucket_idx', 'user_id', user_profile_size_col, user_div_col]\n",
    "    # user_profile_div_df = pd.DataFrame(columns=user_profile_cols)\n",
    "    user_profile_div = np.empty((0, len(user_profile_cols)), dtype=object)\n",
    "\n",
    "    for u in (model_data.userset):\n",
    "        # df (bucket_idx, u, n_seen_items, sum=diversity)\n",
    "        user_profile = model_data.GetUserItems(u, internal=False)\n",
    "        # div = meta_df.loc[meta_df['item_id'].isin(user_profile), div_col].sum()\n",
    "        div = meta_df.loc[meta_df['item_id'].isin(user_profile), user_div_col].sum()\n",
    "        user_profile_div = np.vstack([user_profile_div, np.array([bucket_idx, u, len(user_profile), div])])\n",
    "\n",
    "    return pd.DataFrame(user_profile_div, columns=user_profile_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_column_binned(df:pd.DataFrame, col_name:str, cut:bool=False):\n",
    "    ''' \n",
    "        this function adds a new column with binned values, of the given column, to the given dataframe\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        df[col_name] = df[col_name].astype(int)\n",
    "    except:\n",
    "        print('@get_column_binned(): Aborted! Needs to be a column dtype int!')\n",
    "        raise\n",
    "\n",
    "    if cut:\n",
    "        try:\n",
    "            _, b = pd.cut(df[col_name], 4, retbins=True, labels=False)\n",
    "        except:\n",
    "            try:\n",
    "                _, b = pd.cut(df[col_name], 3, retbins=True, labels=False)\n",
    "            except:\n",
    "                _, b = pd.cut(df[col_name], 4, retbins=True, labels=False, duplicates='drop')\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            _, b = pd.qcut(df[col_name], 4, retbins=True, labels=False)\n",
    "        except:\n",
    "            try:\n",
    "                _, b = pd.qcut(df[col_name], 3, retbins=True, labels=False)\n",
    "            except:\n",
    "                _, b = pd.qcut(df[col_name], 4, retbins=True, labels=False, duplicates='drop')\n",
    "\n",
    "    col_bin_name = col_name+'_binned'\n",
    "    df[col_bin_name] =  b[1:][_]\n",
    "\n",
    "    return col_bin_name #, df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_avg_recdiv_per_bin(df:pd.DataFrame, rec_div_col:str, bin_col:str):\n",
    "    '''\n",
    "       get average diversity of the recommended lists in each bin\n",
    "    '''\n",
    "    try:\n",
    "        df[rec_div_col] = df[rec_div_col].astype(int)\n",
    "    except:\n",
    "        print('@get_avg_recdiv_per_bin(): Aborted! Needs to be a column dtype int!')\n",
    "        raise\n",
    "\n",
    "    avg_recdiv_bin_col = 'avg_'+rec_div_col+'_per_'+bin_col\n",
    "    avg_recdiv_per_bin = df[['bucket_idx','holdout_idx',bin_col, rec_div_col]]\\\n",
    "                            .groupby(['bucket_idx','holdout_idx', bin_col])\\\n",
    "                                .mean().reset_index()\n",
    "    avg_recdiv_per_bin.columns = ['bucket_idx','holdout_idx',bin_col, avg_recdiv_bin_col]\n",
    "    return avg_recdiv_per_bin, avg_recdiv_bin_col\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validate_folderpath(folderpath:str):\n",
    "    if not os.path.exists(folderpath):\n",
    "        os.makedirs(folderpath)\n",
    "\n",
    "\n",
    "def save_plot(title:str, bi:int, hj:int, df:pd.DataFrame, x:str, y:str, path:str):\n",
    "    full_title = title+'- B'+str(bi)+'H'+str(hj)\n",
    "    # islice = pd.IndexSlice\n",
    "    # ax = df.loc[islice['b'+str(bi), 'h'+str(hj), :], [x, y]]\\\n",
    "    # _ = df.loc[(df.bucket_idx=='b'+str(bi)) & (df.holdout_idx=='h'+str(hj)), :]\n",
    "    ax = df.loc[(df.bucket_idx=='b'+str(bi)) & (df.holdout_idx=='h'+str(hj)), [x, y]]\\\n",
    "                .plot(  x=x,\n",
    "                        y=y,\n",
    "                        kind='scatter',\n",
    "                        marker='+',\n",
    "                        # xunits=UnitData(list(_[x].unique())),\n",
    "                        title=full_title)\n",
    "    fig = ax.get_figure()\n",
    "    fig.savefig(path+full_title+'.png')\n",
    "    plt.close(fig)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'UKNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folderpath = '../datasets/goodreads/'\n",
    "\n",
    "dataset_name = 'Goodreads'\n",
    "filename = 'inter_dedup_coldstart_3stars_4x714k'\n",
    "\n",
    "period=['2012-07', '2015-01']\n",
    "sample = 'sample_'+str(period[0])+'_until_'+str(period[1])\n",
    "\n",
    "\n",
    "dump_foldername = 'goodreads_dump/'\n",
    "_, base_outputpath, _, _, _ = get_folderpaths(dump_foldername)\n",
    "\n",
    "sample_version_dump_foldername ='goodreads_dump/'+'pos_rates_only/'+sample+'/'\n",
    "# rule: what/which_data_set/sample_version/what/\n",
    "images_path, output_path, heatmaps_path, diversity_graphpath, diversity_filepath = get_folderpaths(sample_version_dump_foldername)\n",
    "\n",
    "model_diversity_filepath = diversity_filepath+MODEL_NAME+'/'+filename+'_'\n",
    "model_diversity_graphpath = diversity_graphpath+MODEL_NAME+'/'+filename+'_'\n",
    "\n",
    "\n",
    "n_holdouts = 5\n",
    "sample_year_month = [(period[0], '%Y-%m'), (period[1], '%Y-%m')]\n",
    "interval_type = 'S'\n",
    "drop_user_empty_profiles = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_col = 'n_genre' # TODO: UNCOMMENT\n",
    "metainfo_col = 'genres'\n",
    "item_div_col = 'item_'+div_col\n",
    "item_meta_onecol_df = pd.read_csv(base_outputpath+'item_meta_onecol_df.csv', index_col=0)\n",
    "item_meta_onecol_df.columns = ['item_id', metainfo_col, item_div_col]\n",
    "# item_meta_onecol_df.head()\n",
    "\n",
    "meta_df = item_meta_onecol_df.copy()\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAGE: load all recommended lists to frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "print('STAGE: load all recommended lists to frame')\n",
    "all_reclist_df = read_all_reclists_to_frame(filename=model_diversity_filepath+'rec_lists',\n",
    "                                            n_holdouts=n_holdouts)\n",
    "\n",
    "print('does the meta dataframe covers all items recommended?', len(set(meta_df.item_id.unique()).intersection(all_reclist_df.item_id.unique())) == all_reclist_df.item_id.nunique())\n",
    "\n",
    "total_n_reclists = len(all_reclist_df.index.drop_duplicates())\n",
    "# n_holdouts = len(all_reclist_df.index.levels[0])\n",
    "\n",
    "print('Create meta_recommendations frame')\n",
    "meta_rec_df = pd.merge(all_reclist_df.reset_index(), \n",
    "                    meta_df, \n",
    "                    how='left')\n",
    "\n",
    "\n",
    "print('fill in Na values')\n",
    "meta_rec_df.loc[meta_rec_df['item_id'].isna(), 'item_id'] = '0'\n",
    "meta_rec_df.loc[meta_rec_df[item_div_col].isna(), item_div_col] = 0\n",
    "meta_rec_df.loc[meta_rec_df[metainfo_col].isna(), [metainfo_col]] = '0'\n",
    "\n",
    "print('scale diversity column between 0 and 1')\n",
    "item_div_col_minmax = item_div_col+'_minmax'\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "meta_rec_df[item_div_col_minmax] = scaler.fit_transform(meta_rec_df[[item_div_col]])\n",
    "\n",
    "# bc we have all values in one df it won't make a difference\n",
    "# item_div_col_perc = item_div_col+'_perc'\n",
    "# meta_rec_df[item_div_col_perc] = meta_rec_df[item_div_col]/meta_rec_df[item_div_col].max()\n",
    "\n",
    "joblib.dump(meta_rec_df, diversity_filepath+filename+'_meta_rec_df.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_rec_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAGE: calculate the recommended lists diversity \n",
    "(aka intra list diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat_div_col = 'repeat_'+div_col\n",
    "unique_div_col = 'unique_'+div_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unique count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "print('STAGE: calculate the recommended lists diversity aka intra list diversity')\n",
    "print('Note: this is a 1st approach to measuring diveristy, We are only counting the number of different values  ')\n",
    "\n",
    "# this df will sum all genres occurences in the items, considering repeated genres\n",
    "# _ = meta_rec_df[['bucket_idx','holdout_idx','reclist_idx','item_id' ,item_div_col]].drop_duplicates()\n",
    "# reclist_repeatdiv_df = _[['bucket_idx','holdout_idx','reclist_idx',item_div_col]]\\\n",
    "#                 .groupby(['bucket_idx','holdout_idx','reclist_idx'])\\\n",
    "#                     .sum().reset_index()\n",
    "# rec_repdiv_col = 'intra_list_'+repeat_div_col\n",
    "# reclist_repeatdiv_df.columns = ['bucket_idx','holdout_idx','reclist_idx', rec_repdiv_col]\n",
    "\n",
    "\n",
    "# this df will sum all different genres occurences in the list, NOT considering repeated genres\n",
    "reclist_uniquediv_df = meta_rec_df[['bucket_idx','holdout_idx','reclist_idx',metainfo_col]]\\\n",
    "            .drop_duplicates()\\\n",
    "                .groupby(['bucket_idx','holdout_idx','reclist_idx'])\\\n",
    "                    .count().reset_index()\n",
    "rec_uniqdiv_col = 'intra_list_'+unique_div_col\n",
    "reclist_uniquediv_df.columns = ['bucket_idx','holdout_idx','reclist_idx', rec_uniqdiv_col]\n",
    "\n",
    "\n",
    "# reclist_div_df = pd.merge(reclist_repeatdiv_df, reclist_uniquediv_df)\n",
    "reclist_div_df = reclist_uniquediv_df\n",
    "\n",
    "print('scale intra list diversity column between 0 and 1')\n",
    "# rec_repdiv_col_minmax = rec_repdiv_col+'_minmax'\n",
    "# scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# reclist_div_df[rec_repdiv_col_minmax] = scaler.fit_transform(reclist_div_df[[rec_repdiv_col]])\n",
    "\n",
    "\n",
    "rec_uniqdiv_col_minmax = rec_uniqdiv_col+'_minmax'\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "reclist_div_df[rec_uniqdiv_col_minmax] = scaler.fit_transform(reclist_div_df[[rec_uniqdiv_col]])\n",
    "\n",
    "joblib.dump(reclist_div_df, diversity_filepath+filename+'_reclist_div_df.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reclist_div_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAGE: create diversity df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_diversity_count(df:pd.DataFrame, user_list:list, col:str):\n",
    "    return df.loc[df['item_id'].isin(user_list), [col]].drop_duplicates().nunique()[0]\n",
    "    \n",
    "def get_repeat_diversity_count(df:pd.DataFrame, user_list:list, col:str):\n",
    "    return df.loc[df['item_id'].isin(user_list), col].drop_duplicates().sum()\n",
    "\n",
    "\n",
    "def count_diversity_df_NA(div_df:pd.DataFrame, rec_div_col:str, rec_div_col_minmax:str, user_profile_size_col:str, user_div_col:str, drop_user_empty_profiles:bool):\n",
    "    print('\\nNA count')\n",
    "    print('rec_div_col ('+str(rec_div_col)+') ',div_df[rec_div_col].isna().sum())\n",
    "    print('rec_div_col_minmax ('+str(rec_div_col_minmax)+') ',div_df[rec_div_col_minmax].isna().sum())\n",
    "    print('user_profile_size_col ('+str(user_profile_size_col)+') ',div_df[user_profile_size_col].isna().sum())\n",
    "    print('user_div_col ('+str(user_div_col)+') ',div_df[user_div_col].isna().sum())\n",
    "\n",
    "    if drop_user_empty_profiles:\n",
    "        div_df = div_df.loc[~div_df[user_profile_size_col].isna(), :]\n",
    "    else:\n",
    "        for col in [rec_div_col, rec_div_col_minmax, user_profile_size_col, user_div_col]:\n",
    "            div_df.loc[div_df[col].isna(), col] = '0'\n",
    "\n",
    "    print('\\n')\n",
    "    print('rec_div_col ('+str(rec_div_col)+') ',div_df[rec_div_col].isna().sum())\n",
    "    print('rec_div_col_minmax ('+str(rec_div_col_minmax)+') ',div_df[rec_div_col_minmax].isna().sum())\n",
    "    print('user_profile_size_col ('+str(user_profile_size_col)+') ',div_df[user_profile_size_col].isna().sum())\n",
    "    print('user_div_col ('+str(user_div_col)+') ',div_df[user_div_col].isna().sum())\n",
    "\n",
    "    return div_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 165min\n",
    "#########################################################################################\n",
    "print('STAGE: create diversity df')\n",
    "\n",
    "# sort the frame, so the user in the holdout paired with the respective recommended list\n",
    "# (so the order of the holdouts user list is compatible with it)\n",
    "reclist_div_df['reclist_idx'] = reclist_div_df['reclist_idx'].astype(int)\n",
    "reclist_div_df.set_index(['bucket_idx','holdout_idx','reclist_idx'], inplace=True)\n",
    "reclist_div_df.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# user_repdiv_col = 'user_'+repeat_div_col\n",
    "# user_profile_size_col = 'n_seen_items'\n",
    "# user_profile_cols = ['bucket_idx', 'user_id', user_profile_size_col, user_repdiv_col]\n",
    "# user_profile_div = np.empty((0, len(user_profile_cols)), dtype=object)\n",
    "\n",
    "user_uniqdiv_col = 'user_'+unique_div_col\n",
    "user_profile_size_col = 'n_seen_items'\n",
    "user_profile_cols = ['bucket_idx', 'user_id', user_profile_size_col, user_uniqdiv_col]\n",
    "user_profile_div = np.empty((0, len(user_profile_cols)), dtype=object)\n",
    "\n",
    "\n",
    "# index slice to access multi-index\n",
    "islice = pd.IndexSlice\n",
    "\n",
    "# initialize user id column\n",
    "reclist_div_df['user_id'] = None\n",
    "\n",
    "holdouts = joblib.load(output_path+filename+'_semesterly_holdouts.joblib')\n",
    "for hj, holdoutj in enumerate(holdouts): \n",
    "\n",
    "    # load user profile @holdout_j -> read model that has seen all *buckets* until *jth holdout*, aka bj\n",
    "    model_data_bucketj = joblib.load(model_diversity_filepath+'model_data_b'+str(hj)+'.joblib')\n",
    "    print('\\tcalculate user diversity @'+str(hj))\n",
    "    for u in (model_data_bucketj.userset):\n",
    "        # df (bucket_idx, u, n_seen_items, sum=diversity)\n",
    "        user_profile = model_data_bucketj.GetUserItems(u, internal=False)\n",
    "\n",
    "        div = get_unique_diversity_count(meta_df, user_profile, metainfo_col)\n",
    "        # div = get_repeat_diversity_count(meta_df, user_profile, item_div_col)\n",
    "        \n",
    "        user_profile_div = np.vstack([user_profile_div, np.array(['b'+str(hj), u, len(user_profile), div])])\n",
    "\n",
    "    print('\\tadd user to the respective rec lists @'+str(hj))\n",
    "    for bi in range(n_holdouts):\n",
    "        # reclist_div_df['user_id'] = h.userlist\n",
    "        reclist_div_df.loc[islice['b'+str(bi), 'h'+str(hj), :], 'user_id'] = list(holdoutj.userlist)\n",
    "    \n",
    "\n",
    "# merge the user profile info to the diversity df\n",
    "user_profile_div_df = pd.DataFrame(user_profile_div, columns=user_profile_cols)\n",
    "diversity_df = pd.merge(reclist_div_df.reset_index(),\n",
    "                        user_profile_div_df,\n",
    "                        how='left')\n",
    "\n",
    "\n",
    "diversity_df = count_diversity_df_NA(diversity_df, \n",
    "                                     rec_uniqdiv_col, \n",
    "                                     rec_uniqdiv_col_minmax, \n",
    "                                     user_profile_size_col, \n",
    "                                     user_uniqdiv_col, \n",
    "                                     False)\n",
    "\n",
    "joblib.dump(reclist_div_df, diversity_filepath+filename+'_diversity_df.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAGE: bin columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "print('STAGE: bin columns - Qcut')\n",
    "\n",
    "# qcut function requires an int type column, binning the standardized column will put all values in the 0 bin\n",
    "\n",
    "# USER PROFILE\n",
    "user_profile_size_bin_col = get_column_binned(diversity_df, user_profile_size_col)\n",
    "\n",
    "# min max\n",
    "# _, avg_recrepdiv_profilebin_col_minmax = get_avg_recdiv_per_bin(diversity_df,\n",
    "#                                                              rec_repdiv_col_minmax, \n",
    "#                                                              user_profile_size_bin_col)\n",
    "# diversity_df = pd.merge(diversity_df, _)\n",
    "\n",
    "# not scaled\n",
    "_, avg_recrepdiv_profilebin_col = get_avg_recdiv_per_bin(diversity_df,\n",
    "                                                      rec_uniqdiv_col, \n",
    "                                                      user_profile_size_bin_col)\n",
    "diversity_df = pd.merge(diversity_df, _)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# USER DIVERSITY\n",
    "user_repdiv_bin_col = get_column_binned(diversity_df, user_uniqdiv_col)\n",
    "\n",
    "# minmax\n",
    "# _, avg_recrepdiv_userrepdivbin_col_minmax = get_avg_recdiv_per_bin(diversity_df,\n",
    "#                                                                 rec_repdiv_col_minmax, \n",
    "#                                                                 user_repdiv_bin_col)\n",
    "# diversity_df = pd.merge(diversity_df, _)\n",
    "\n",
    "# not scaled\n",
    "_, avg_recrepdiv_userrepdivbin_col = get_avg_recdiv_per_bin(diversity_df,\n",
    "                                                        rec_uniqdiv_col, \n",
    "                                                        user_repdiv_bin_col)\n",
    "diversity_df = pd.merge(diversity_df, _)\n",
    "\n",
    "\n",
    "joblib.dump(diversity_df, diversity_filepath+filename+'_diversity_df+binned_cols.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAGE: plot diversity values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_diversity_graphpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "print('STAGE: plot diversity values')\n",
    "for bi in range(n_holdouts):\n",
    "    for hj in range(n_holdouts):\n",
    "        # save_plot(title='Profile size vs Avg intra-list diversity (standardized)',\n",
    "        #           bi=bi,\n",
    "        #           hj=hj,\n",
    "        #           df=diversity_df,\n",
    "        #           x=user_profile_size_bin_col,\n",
    "        #           y=avg_recrepdiv_userrepdivbin_col_minmax,\n",
    "        #           path=model_diversity_graphpath)\n",
    "        \n",
    "        save_plot(title='Profile size vs Avg intra-list diversity',\n",
    "                  bi=bi,\n",
    "                  hj=hj,\n",
    "                  df=diversity_df,\n",
    "                  x=user_profile_size_bin_col,\n",
    "                  y=avg_recrepdiv_userrepdivbin_col,\n",
    "                  path=model_diversity_graphpath)\n",
    "        \n",
    "        save_plot(title='User diversity vs Avg intra-list diversity',\n",
    "                  bi=bi,\n",
    "                  hj=hj,\n",
    "                  df=diversity_df,\n",
    "                  x=user_repdiv_bin_col,\n",
    "                  y=avg_recrepdiv_userrepdivbin_col,\n",
    "                  path=model_diversity_graphpath)\n",
    "        \n",
    "        # save_plot(title='User diversity vs Avg intra-list diversity (standardized)',\n",
    "        #           bi=bi,\n",
    "        #           hj=hj,\n",
    "        #           df=diversity_df,\n",
    "        #           x=user_repdiv_bin_col,\n",
    "        #           y=avg_recrepdiv_userrepdivbin_col_minmax,\n",
    "        #           path=diversity_graphpath)                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m_f_1st",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
